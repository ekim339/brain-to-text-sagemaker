# Example: Stepwise Learning Rate Decay Configuration
# Add these parameters to your config file

# Option 1: StepLR - Decays by a factor every N steps
lr_scheduler_type: step  # Use 'step' for StepLR
lr_max: 0.001  # Starting learning rate
lr_step_size: 50000  # Decay every 50,000 steps
lr_decay_factor: 0.1  # Multiply LR by 0.1 at each step
# Example: LR = 0.001 → 0.0001 (step 50k) → 0.00001 (step 100k) → 0.000001 (step 150k)

# Option 2: MultiStepLR - Decays at specific milestone steps
lr_scheduler_type: multistep  # Use 'multistep' for MultiStepLR
lr_max: 0.001  # Starting learning rate
lr_milestones: [50000, 100000, 130000]  # Decay at these specific steps
lr_decay_factor: 0.1  # Multiply LR by 0.1 at each milestone
# Example: LR = 0.001 → 0.0001 (step 50k) → 0.00001 (step 100k) → 0.000001 (step 130k)

# Note: lr_min, lr_decay_steps are not used for stepwise decay
# The decay continues until training ends (no minimum enforced)

