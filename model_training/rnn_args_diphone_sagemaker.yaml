model:
  n_input_features: 512 # number of input features in the neural data
  n_units: 1024 # number of units per GRU layer (increased for diphones)
  rnn_dropout: 0.5 # dropout rate for the GRU layers
  rnn_trainable: true
  n_layers: 5 # number of GRU layers
  bidirectional: true # Set to true for bidirectional RNN (doubles params, no real-time use)
  patch_size: 14 # size of the input patches
  patch_stride: 4 # stride for the input patches

  input_network:
    n_input_layers: 1
    input_layer_sizes:
    - 512
    input_trainable: true
    input_layer_dropout: 0.3 # increased for regularization

gpu_number: '0' # SageMaker sets this automatically
mode: train
use_amp: false # Disabled for stability with 1681 classes (float16 can be unstable)

# These will be overridden by train_model_sagemaker.py
output_dir: /opt/ml/output
checkpoint_dir: /opt/ml/model/checkpoint
init_from_checkpoint: false
init_checkpoint_path: None

save_best_checkpoint: true
save_all_val_steps: false
save_final_model: false
save_val_metrics: true
early_stopping: true
early_stopping_val_steps: 20

num_training_batches: 150000 # increased for diphone training
lr_scheduler_type: cosine
lr_max: 0.001 # VERY low for bidirectional stability (2Ã— params = more sensitive)
lr_min: 0.0005
lr_decay_steps: 150000
lr_warmup_steps: 5000 # Long warmup for bidirectional stability
lr_max_day: 0.005 # reduced to match lr_max
lr_min_day: 0.0005
lr_decay_steps_day: 150000
lr_warmup_steps_day: 5000 # match main warmup steps

# lr_scheduler_type: step  # Use 'step' for StepLR
# lr_max: 0.001  # Starting learning rate
# lr_step_size: 50000  # Decay every 50,000 steps
# lr_decay_factor: 0.1  # Multiply LR by 0.1 at each step

beta0: 0.9
beta1: 0.999
epsilon: 1.0e-8  # Fixed! Was 0.1 which caused NaN gradients
weight_decay: 0.0005 # Reduced for stability (was 0.002)
weight_decay_day: 0
seed: 10
grad_norm_clip_value: 10 # Moderate clipping (allows higher LR with batch_size=16)

# Composite loss: L = alpha * (phoneme_loss) + (1 - alpha) * (diphone_loss)
use_composite_loss: true  # ENABLED - train with both diphone and phoneme losses
composite_loss_alpha: 0.6  # Weight for phoneme loss (0.0 = only diphone, 1.0 = only phoneme) [ignored if using schedule]

# Dynamic alpha schedule (curriculum learning: gradually increase phoneme weight)
# Paper schedule: First 10 epochs diphone-only, then +0.1 every 10 epochs until 0.6
# To match paper: estimate your epoch size (e.g., 1000-2000 batches) and multiply by 10
use_alpha_schedule: true  # ENABLED - use curriculum learning
alpha_schedule_start: 0.0  # Starting alpha (0.0 = diphone-only)
alpha_schedule_end: 0.6  # Maximum alpha (0.6 = 60% phoneme, 40% diphone)
alpha_schedule_step_size: 0.1  # Increase alpha by this amount
alpha_schedule_step_interval: 10000  # Paper uses ~10 epochs; adjust based on your epoch size

batches_per_train_log: 200
batches_per_val_step: 1500 # more frequent validation

batches_per_save: 0
log_individual_day_val_PER: true
log_val_skip_logs: false
save_val_logits: true
save_val_data: false

dataset:
  data_transforms:
    white_noise_std: 1.0
    constant_offset_std: 0.2
    random_walk_std: 0.0
    random_walk_axis: -1
    static_gain_std: 0.0
    random_cut: 3
    smooth_kernel_size: 100
    smooth_data: true
    smooth_kernel_std: 2

  neural_dim: 512
  batch_size: 32 # Further reduced for stability (was 48)
  n_classes: 1681 # DIPHONES (was 41 phonemes)
  max_seq_elements: 500
  days_per_batch: 4
  seed: 1
  num_dataloader_workers: 4 # parallel S3 reads (reduce to 2 if issues)
  loader_shuffle: false
  must_include_days: null
  test_percentage: 0.1
  feature_subset: null
  preprocessing: true
  bin_size: 2

  # This will be overridden to use S3 paths
  dataset_dir: /dummy
  bad_trials_dict: null
  
  sessions:
  - t15.2023.08.11
  - t15.2023.08.13
  - t15.2023.08.18
  - t15.2023.08.20
  - t15.2023.08.25
  - t15.2023.08.27
  - t15.2023.09.01
  - t15.2023.09.03
  - t15.2023.09.24
  - t15.2023.09.29
  - t15.2023.10.01
  - t15.2023.10.06
  - t15.2023.10.08
  - t15.2023.10.13
  - t15.2023.10.15
  - t15.2023.10.20
  - t15.2023.10.22
  - t15.2023.11.03
  - t15.2023.11.04
  - t15.2023.11.17
  - t15.2023.11.19
  - t15.2023.11.26
  - t15.2023.12.03
  - t15.2023.12.08
  - t15.2023.12.10
  - t15.2023.12.17
  - t15.2023.12.29
  - t15.2024.02.25
  - t15.2024.03.03
  - t15.2024.03.08
  - t15.2024.03.15
  - t15.2024.03.17
  - t15.2024.04.25
  - t15.2024.04.28
  - t15.2024.05.10
  - t15.2024.06.14
  - t15.2024.07.19
  - t15.2024.07.21
  - t15.2024.07.28
  - t15.2025.01.10
  - t15.2025.01.12
  - t15.2025.03.14
  - t15.2025.03.16
  - t15.2025.03.30
  - t15.2025.04.13
  
  dataset_probability_val:
  - 0 # no val or test data from this day
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 0 # no val or test data from this day
  - 1
  - 1
  - 1
  - 0 # no val or test data from this day
  - 0 # no val or test data from this day
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1

